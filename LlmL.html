<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="description" content="LlmL | The personal blog of Veit Heller">
    <meta name="author" content="Veit Heller">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LlmL | Veit's Blog</title>
    <link rel="stylesheet" type="text/css" href="./le_style.css">
		 <script src="./le_script.js"></script>
    <link rel="icon" href="data:;base64,iVBORw0KGgo=">
  </head>
  <body lang="en">
    <div class="aside">
        <p><a href="/">Veit's Blog</a></p>
        <h1>LlmL</h1>
        <p class="date">May 29 2025</p>
    </div>
    <div class="content">
      <div class="snippet textual-snippet text-snippet paragraphWidth">  <p>For a while now, one of my biggest responsibilities in <a href="https://gtoolkit.com/">Glamorous Toolkit</a> has been to develop <a href="https://github.com/feenkcom/gt4llm">gt4llm</a>, a workbench for large language models that is intended to help develop assistants and also comes with a set of integrated assistants for code, prose, and other subsystems with GT.  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>I haven’t written much about the workbench outside of the Glamorous Toolkit book, and if you want to learn more about it, <a href="https://book.gtoolkit.com/gt4llm-270ytb3y5mi1voswipfyc5hti">I suggest reading there</a>. It has all the context I know to talk about<sup><a href="#1">1</a></sup>.  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>Instead, today I want to talk about a silly little idea that I had a long while back, and finally realized using the workbench: LlmL, the language that lets you shell out to an LLM when you don’t know how to solve a problem<sup><a href="#2">2</a></sup>. Let’s start by looking at the language, and then talk a bit about the implementation, and see what we can learn from it. Beware that the implementation bit is written in GT-specific Smalltalk, but the code should be short and simple enough that the idea can get across.  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <h4 id="a-really-dumb-language-meets-a-really-dumb-idea">    <a href="#a-really-dumb-language-meets-a-really-dumb-idea">  A really dumb language meets a really dumb idea    </a>  </h4></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>The language "design" of LlmL was primarily shaped by the one hour timebox I set for myself when working on this project. As such, It only has numbers as types, and it uses an S-expression-based syntax (for ease of parsing).  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>Here is a sample:  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>    <div class="code_block">      <pre><code class="smalltalk">(def x
  (fn (n)
    (* n 2)))
(+ (x 10) 3)</code></pre>    </div>  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>This should result in <code>23</code>.  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>So far, so simple. We have a language with functions, variables and numbers, using an S expression syntax. So, what is different?  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>Well, consider factorial. How would you usually write factorial in a language like that? Here is one way:  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>    <div class="code_block">      <pre><code class="smalltalk">(def factorial
  (fn (n)
    (if (= n 0)
      1
      (* n (factorial (- n 1))))))

(factorial 5)</code></pre>    </div>  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>Except this wouldn’t work, because we don’t have <code>if</code> and conditionals and all that jazz. Instead of trying to implement all of that using Church encodings and fancy lambda calculus thinga<sup><a  href="#3">3</a></sup>, we can instead just call <code>infer</code>.  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>    <div class="code_block"><pre><code class="smalltalk">(def factorial (infer))(factorial 5)</code></pre>    </div>  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>And it will give us the right answer, 120!  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>So, how does that work? Well, the basic idea was that in the age of just blindly copying a bunch of code that a language model generates and putting that in our code base, how about we just cut out the middleman and just ask it for the answer at runtime instead?  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>So, we give the LLM the name of the function as well as the arguments it was called with and ask it to generate an answer, and then pass that back into the interpreter. Cool, right?  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>I think we just accidentally stumbled into the future, where solving problems is outsourced in real-time.  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <h4 id="how-do-llml-do">    <a href="#how-do-llml-do"> How do LlmL do?    </a>  </h4></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>Now that we’ve talked about     <i>what    </i> LlmL is, let’s talk about how it works.  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>The parser uses a framework called SmaCC that is used for virtually every parser in GT. It’s a parser generator framework and it’s quite awesome. Once you know how to wield it (which might take a bit), it takes something like 5-10 minutes to write a parser for something like LlmL that produces a neat little AST.  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>From there, we build a simple evaluator that is just a tree-walking interpreter that keeps track of the variables in an environment and allows for environments to be pushed and popped as needed.  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>Really the only novel and mildly interesting bit is inside the evaluation for the <code>infer</code> primitive, which in turn happens in a class called <code class="annotation annotation-label annotation-label-class annotation-label-code expandable-annotation-label annotation-sibling-is-collapsed">LlmlInfer</code>    <span class="textual-snippet code-snippet pharo-class-definition expandable-annotation-view collapsed-annotation-view"><code class="">Object subclass: #LlmlInfer	instanceVariableNames: 'connection'	classVariableNames: ''	package: 'LlmL-Model'</code>    </span>.  Its method <code class="annotation annotation-label annotation-label-method annotation-label-code expandable-annotation-label annotation-sibling-is-collapsed">LlmlInfer>>#evaluate:in:</code>    <span class="textual-snippet code-snippet pharo-method-snippet expandable-annotation-view collapsed-annotation-view"><code class="">evaluate: aCollection in: aLlmlEvaluator	| chat |	chat := GtLlmChat new			provider: (self connection buildProvider format: self format).	chat		sendMessage: 'Provide the output of the following expression: '				, aLlmlEvaluator callStack last source.	chat provider executions last wait.	^ (chat messages last contentJson at: 'result') asNumber</code>    </span> is the key to the LLM connection.  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>It uses <a href="https://openai.com/index/introducing-structured-outputs-in-the-api/">structured outputs</a> to guarantee that we always get a well-formed result. For those not in the know, it’s basically a way to make the LLM to adhere to a user-controlled JSON schema in the output.  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>To transcribe it as a snippet and annotate, it basically does this:  </p></div><div class="snippet textual-snippet code-snippet pharo-snippet fullWidth">  <pre class="no_bottom_margin">"Providers hold onto all the low-level bits and are
different for every LLM API. Currently we support Ollama,
OpenAI, Anthropic, and Google Gemini. These providers also
understand how to add a structured output format to the API
call."provider := self connection buildProvider
  format: self format."Then we create a chat and send a message."chat := GtLlmChat new provider: provider.chat sendMessage:
  'Provide the output of the following expression: ',   aLlmlEvaluator callStack last source.
"^We get the last section from the callstack here.
It also has the arguments.""These executions happen asynchronously, so we force
synchronicity by waiting."chat provider executions last wait."We get the result from the JSON we got back and just
confidently convert it to a number."(chat messages last contentJson at: 'result') asNumber</pre></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>Usually in <code>gt4llm</code>, the format that we would use would be autogenerated by a higher level schema, but I didn’t want to bother with it, so I just wrote a simple structured output format by hand:  </p></div><div class="snippet textual-snippet code-snippet pharo-snippet fullWidth"><pre class="no_bottom_margin">{'type' -> 'object'. 'properties' ->
    {'result' ->
      {'type' -> 'number'} asDictionary} asDictionary.  'required' -> {'result'}.  'additionalProperties' -> false} asDictionary</pre></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>This won’t work with all providers because they are ever so slightly different in how they want their schema to be formatted. It does work with Ollama and OpenAI, though, and that was good enough for me here. For anything more, resort to the higher level.  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>Still, the core of the system works with all providers we’ve provided in <code>gt4llm</code>, so whatever you set up as your default connection will be used by the evaluator by default. You can also override it to inject your own models or providers per evaluation:  </p></div><div class="snippet textual-snippet code-snippet pharo-snippet fullWidth">  <pre class="no_bottom_margin">LlmlEvaluator new	connection: (GtLlmConnection new			provider: GtOllamaProvider;			model: 'llama3.2');	evaluate: (LlmlParser			parse: '(def factorial (infer)) (factorial 4)')</pre></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>And I think that’s all of it!  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <h4 id="fin">    <a href="#fin"> Fin    </a>  </h4></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>I hope you enjoyed this little tour of my groundbreaking new research in programming language design!  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>Okay, admittedly, LlmL is silly, but maybe not as silly as one might think at first glance: when people copy-paste code they found online or that an LLM spit out, what they actually want, at the heart of it, is an     <i>oracle    </i>. Someone to just tell them the answer. And instead of generating an algorithm that won’t be reviewed anyway, this just gives you the answer right away. But it’s also a reductio ad absurdum of this idea that, somehow, this will solve our problems, because it will not. It just creates the ultimate black box.  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p>I really wanted to write a more philosophical article about LLMs, but I thought you deserved a bit of technical whimsy before that. I hope you liked the appetizer, the main course is to follow!  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <h6 id="footnotes">    <a href="#footnotes"> Footnotes    </a>  </h6></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p><span id="1">1.</span> If you find anything missing, do not hesitate to contact me or <a href="https://github.com/feenkcom/gtoolkit/issues/">file an issue</a>!  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p><span id="2">2.</span> For a more manual way of approaching problem solving, <a href="https://blog.veitheller.de/On_starting_hard_things.html">read my last blog post</a>.  </p></div><div class="snippet textual-snippet text-snippet paragraphWidth">  <p><span id="3">3.</span> You couldn’t actually do that, either, because you’d have to start with a Church encoding instead of numbers to get to equality. If you have equality, it’s all smooth sailing from there.  </p></div>
      <div class="footer">
        <p>Want to go back to the <a href="/">list of posts</a>?</p>
      </div>
    </div>
  </body>
</html>
